1到5章：机器学习基础；剩21天

6到10章：监督学习的参数化模型 23天

11到13章：监督学习的非参数化模型 

14到18章：无监督模型         （两部分合计32天） 

预计元旦前结束学习

机器学习

​	1.	线性回归、逻辑斯谛回归和双线性模型（第1、4、5章）：理解线性分类的基本原理。

​	2.	支持向量机（第9章）：深入理解SVM的工作原理和分类边界优化。

​	3.	决策树与集成学习（第10、11章）：掌握数模型的构建方式及其在分类和回归任务中的应用。

	4.	概率图模型（第14章）：了解图模型如何表示复杂的概率关系。

深度学习

​	1.	第14章：词向量和预训练的基础知识。

​	2.	第8章：基础循环神经网络（RNN）。

​	3.	第9章：LSTM、GRU和双向RNN，理解更先进的RNN变体。

​	4.	第10章：注意力机制，为进一步学习更复杂的NLP模型做准备。

# 第一章 初探机器学习



## 1.1 人工智能的两只手和四条腿

人工智能有“两只手和四条腿”。“两只手”代表的是人工智能可以做的两大类任务，即**预测**与**决策**。

人工智能中服务**预测**任务的是**机器学习**技术。而支撑**决策**任务的机器学习技术被称为**强化学习（reinforcement learning）**，当然，强化学习可以看作是机器学习的一部分

## 1.2 机器学习是什么

- "学习"是什么

用比较计算机的语言来描述，**学习就是系统基于数据来提升既定指标分数的过程**。

- 定义机器学习

机器学习是一门研究算法的学科，这些算法能够通过**非显式编程（non-explicit programming）**的形式在某个任务上，通过经验数据，来提升性能指标。

数学描述：

使用一个优化问题来刻画。针对某一预测任务，其数据集为 $D$，对于一个机器学习预测模型$f$，预测任务的性能指标可以通过一个函数来表示 $P(D, f)$，那么机器学习的过程则是在一个给定的模型空间 $A$ 中，寻找可以最大化性能指标的预测模型 $f^*$ ：
$$
f^* = arg~\mathop{max}_{f\in A} {P(D, f)} = ML(D)
$$
f^* = arg~\mathop{max}_{f\in A} {P(D, f)} = ML(D)$

这里的 $ML(D)$ 则表示机器学习可以被看成是一个输入数据集、输出解决任务算法的算法。

- 非显式编程

首先，**显式编程**需要开发者首先自己可以完成该智能任务，才能通过实现对应的逻辑来使机器完成它，相当于要事先知道了$f^*$，然后直接实现它。

但有很多智能任务是无法通过直接编程来实现的，如人脸识别。

具体地，在上述优化范式中，我们在一个模型空间 $A$ 中寻找最优模型的过程 $f^*$ ，可以是一个持续迭代的形式，即
$$
f_0\rightarrow f_1 \rightarrow f_2 \rightarrow · · · \rightarrow f^*
$$
而这个寻找最优模型 $f^*$ 的过程，就是机器学习。机器学习的算法可以对应从 $f_{i}$ 迭代到 $f_{i+1}$ 的程序。

因为有了机器学习技术，只需要拥有任务的数据，就可以得到解决任务的算法。这样，程序员就可以“往后站一步”，从直接编写各类任务具体的算法代码，转为编写机器学习算法代码，然后在不同任务中，基于任务自身的数据，学习出一个解决该任务的算法（即机器学习模型）出来，如图所示。

![截屏2024-10-13 17.29.30](/Users/aris/Library/Application Support/typora-user-images/截屏2024-10-13 17.29.30.png)

- 时代造就机器学习的盛行

那么机器学习技术为什么现在如此盛行呢？主要原因之一来自于时代造就！2006 年，美国亚马逊公司推出云计算服务，云计算技术开始普及，不少公司开始将大量业务数据存储到云平台。2011 年，大数据概念开始深入人心，大量的公司基于云计算平台积累下来的大数据开始挖掘价值，而基于大数据的机器学习技术则开始越来越被重点关注。2012 年，深度学习率先在计算机视觉领域取得大的突破，随后 2013 年的自然语言处理、语音识别和深度强化学习等人工智能分支和应用场景开始大爆发。我们可以看到，在云计算和大数据的基础下，机器学习作为一种在数据和算力给足的情况下可以变得更强大的人工智能技术，获得了充分成长的条件。而到了 2020 年代的今天，我们已经看到机器学习技术开始从数据量最多的互联网场景在大量外溢到传统工业、农业场景，例如工厂的能效优化和排产规划，农田收成预测与种植规划等，各个行业在经历一个数字化转型和智能化升级的阶段。我们有理由相信，未来 10 年，机器学习技术会持续渗透到各行各业，在各类预测和决策任务场景中服务人类。



## 1.3 机器学习的分类

**按照任务来分类**，机器学习可以分为三大类：

- 有监督学习（supervised learning）：训练集 $D$ 中的每一个数据实例 $(x , y)$ 由特征和标签组成。模型的任务是根据数据的特征来预测其标签。模型的性能指标可以由一个损失函数 $L(y, f(x))$ 来定义。该损失函数衡量具体的数据实例 $(x, y)$ 上的预测偏差，性能指标可以定义为损失函数的负数。由此，有监督学习的一般形式可以写为
  $$
  f^* = arg~\mathop {min}_{f}~\frac{1}{|D|}\sum_{(x, y)\in D}L(y, f(x))
  $$
  
- 无监督学习（unsupervised learning）：无监督学习的任务目标多种多样，但大多需要我们学习数据的分布 $p(x)$ 。与有监督学习不同，无监督学习任务中的数据没有标签的概念，或者说，所有数据维度都是同等重要的，也即是训练集 $D$ 的每一个数据实例仅由数据特征 $x$ 来表示。这时我们往往使用概率分布模型 $p(x)$ 来建模数据的分布。在数据实例满足独立同分布的普遍假设下，整个数据集的对数似然（log-likelihood）则作为无监督学习需要最大化的目标，即
  $$
  p^*= arg~\mathop {max}\limits_{p}\frac{1}{|D|}\sum_{x\in D}log~p(x)
  $$

**对比有监督学习和无监督学习**：有监督学习的只关心基于数据特征对标签的预测是否精准，而并不关心数据特征之间的相关性等模式。例如人脸识别就是一个典型的有监督学习任务，模型只关心输入人脸图像后是否能准备预测对应的身份，而不关注输入图像本身是否是包含一个人脸。而无监督学习则关注数据的分布与其中包含的模式，如关注人脸图像的概率分布，并可以判断一张给定的图像是否包含一个人脸，甚至可以生成一张新的人脸图像。本书的前三部分讲述有监督学习的内容，第四部分讲述无监督学习的内容。

- 强化学习（reinforcement learning）：与有监督学习和无监督学习关于人工智能中的预测问题不同，强化学习关注人工智能中的决策问题。强化学习是寻找更好的决策策略的过程，而优化的目标则是策略决策带来的累积回报的期望。由于强化学习的数学建模方式与有监督学习和无监督学习有较大差距，也超出了本书的讨论范畴，因此这里不具体给出数学公式

**按照建模方式来分类**，机器学习模型可以分为参数化模型和非参数化模型两大类：

- 参数化模型（parametric model）：在一套具体的模型族（model family）内，每一个具体的模型都可以用一个具体的参数向量来唯一确定，因此确定了参数向量也就确定了模型。例如，在上述有监督学习中，可以将参数向量 $\theta$ 写到预测模型的下标，即 $f_\theta$ ，来表示参数化模型。因此，参数化模型的监督学习也可以写成是寻找最优参数 $\theta ^*$的过程：

$$
\theta^*= arg~\mathop{min}_{\theta}\frac{1}{|D|}\sum_{(x,y)\in D}L(y, f_\theta(x))
$$

参数化模型的一大性质是，模型的参数量不随着训练数据量而改变。因此，在计算过程中，模型占用计算机的资源（如内存或者显存）是固定的。 本书第一、二部分中讲到的线性回归、逻辑回归、双线性模型、神经网络模型都是典型的参数化模型。求解上述最优参数可以借助损失函数针对模型参数的梯度来完成，方法具有普适性，因此参数化模型整体上比非参数化模型更加普遍，有很多机器学习代码库的支持，包括深度学习框架 PyTorch、TensorFlow 等。

- 非参数化模型（nonparametric model）：与参数化模型相反，非参数化模型并非由一个具体的参数向量来确定，其训练的算法也不是更新模型的参数，而是由具体的计算规则直接在模型空间中寻找模型实例。由于模型和参数并非一一对应，因此数据量的不同（或者数据的不同）会导致模型中具体使用到的参数量也不同。对于有的非参数化模型，例如 KNN、高斯过程，其参数量和训练数据量成正比，即每个数据实例就是一个参数。

KNN、树模型、支持向量机都是极其重要的机器学习模型，并且在实践中具有不可替代的功能。可以理解为，参数化模型将从数据中学到的知识注入到参数中，而非参数化模型则保留数据本身作为知识。本书第三部分会集中讲解非参数化模型。



## 1.4 泛化能力：机器学习奏效的本质

那么为什么机器学习模型通过在有限的数据上训练后，就可以在其他没见过的数据上做出一定精度的预测呢？在机器学习里，“泛化能力”（generalization ability）被用来描述一个智能模型在没见过的数据上的预测性能。一般使用泛化误差来量化一个模型的泛化能力，具体定义为模型在给定数据分布下的损失函数值的期望：
$$

$$


机器学习的底层是数理统计，其基本原理是，相似的数据拥有相似的标签。机器学习模型对于新数据的标签预测的“底气”在于见过类似的数据，这样的泛化能力可以被称为统计泛化。定性来讲，如果预测任务越简单，训练数据量越大，那么学到的机器学习模型的泛化能力就越强。另一方面，选择的机器学习模型的复杂性和其泛化能力并没有直接的对应关系。具体来说，模型空间越复杂，其建模能力越强，但也越需要足够的训练数据来支撑，否则模型可能由于“过拟合”数据而导致其泛化能力低下。



## 1.5 归纳偏置：机器学习模型的天赋

在给定的任务和训练数据集下，不同的机器学习模型训练出来的性能总是不同的，或者反过来说，让不同的机器学习模型的给定任务下达到相同的泛化能力，需要的训练数据量往往也是不同的。这背后的原因是，不同的机器学习模型对特定数据模式的归纳偏置（inductive bias）不同。所谓归纳偏置，就是指模型对问题的先验假设，比如假设空间上相邻的样本有相似的特征。归纳偏置可以让模型在缺乏对样本的知识时也能给出预测，对某类数据的归纳偏置更强的模型能够更快地学到其中的模式。例如神经网络模型对同分布域的感知数据的归纳偏置就很强，处理图像和语音的模式识别任务效果非常好；而树模型对混合离散连续的结构化数据的归纳偏置很强，对于银行表单数据和医疗风险的预测效果很好。可以说，归纳偏置就是机器学习模型的天赋。图 1-2 展示了线性分类模型和圆形分类模型的分类决策边界。可以看出当数据分布在直线两侧时，线性分类模型可以很轻松地将两类数据分开，而当数据呈里外两类分布时，圆形分类模型则能更容易地将两类数据分开，这体现出两个模型归纳偏置的不同。

![截屏2024-10-15 01.29.42](/Users/aris/Library/Application Support/typora-user-images/截屏2024-10-15 01.29.42.png)

因此，对于不同的任务、不同的数据，选择怎样的模型是机器学习算法工程师需要重点考虑的。在企业里，一个好的机器学习算法工程师能根据自己的经验，为公司不同的业务选择适合的模型，这样能提升模型预测的精度，提升业务效率，或者减少模型选型的更迭次数，节约开发成本。



## 1.6 机器学习的限制

至少在目前阶段，机器学习并不是万能的，它在以下几个方面存在限制:

- 数据限制：在某些场景，即使使用了所有的数据，也可能不足以训练出一个令人满意的预测模型。在算力允许的情况下，人们倾向于设计越来越复杂的模型，例如千亿参数级别的线性模型或者深度神经网络模型，但与之匹配的训练数据量却比较难以跟上。以互联网企业用户行为预测任务为例，为了训练复杂度更高的模型，需要匹配更多的训练数据，因而不得不用比较早期的数据来支撑，但用户在互联网平台的行为模式有一定时效性，所以使用较为过时的数据可能并不能帮助模型进一步提升预测精度。
- 泛化能力限制：机器学习的统计泛化能力并非使它无所不能。人工智能场景中有不少任务是缺乏数据或是要求举一反三的，这需要逻辑推理技术的组合泛化能力。因此，我们认为，在通往强人工智能（或称为通用人工智能）的道路上，机器学习是重要但并非唯一需要的技术。现在也有一些新兴的国际研讨会或者学术会议聚焦研究机器学习和符号逻辑的融合方法，旨在让机器同时拥有统计泛化和组合泛化能力，以解决更加复杂的智能任务。
- 使用形态限制：就像上文描述的公式 $f=ML(D)$ 所表达的，目前机器学习主要使用的形态是一个针对特定任务，输入数据集，输出训练好的模型的工具。这距离通用人工智能（或者强人工智能）还有较远的距离。未来的机器学习在具备了更好的性能条件下，可以在不断收集的数据上做持续学习，能自动判别和选择新的需要学习的任务，能从过去多种任务的学习过程中总结更高效的学习方法，在新任务中做到小样本学习，融入人类的知识库并做到“举一反三”的组合泛化。

# 第二章 机器学习的数学基础



## 2.1 向量

- 向量可以用行向量 $x= (x_1, x_2,...,x_n)$ 和列向量（一般情况下用列向量）表示:
  $$
  \begin{pmatrix}
  x_1 \\
  x_2 \\
  \vdots \\
  x_n \\
  \end{pmatrix}
  $$
  为了方便，一般用转置后的行向量表示：
  $$
  x^T = (x_1, x_2,...,x_n)
  $$

- 设向量$x,y\in R^n$，标量$a\in R$，向量的常见运算：

  - 向量加法
  - 向量数乘
  - 向量内积

- 范数

  一个函数 $\|\cdot\|$  是范数，如果它满足以下三个条件：

  - 非负性（Non-negativity）：对于任意向量  $x$ ，有  $\|x\| \geq 0$ ，且当且仅当  $x = 0$  时  $\|x\| = 0$ 。

  - 齐次性（Homogeneity）：对于任意实数 $\alpha$  和向量  $x$ ，有  $\|\alpha x\| = |\alpha| \|x\|$ 。

  - 三角不等式（Triangle Inequality）：对于任意向量  $x$  和  $y$ ，有  $\|x + y\| \leq \|x\| + \|y\|$ 。

  常见的范数类型

  - $L_1$范数（曼哈顿范数）：定义为向量各元素绝对值之和

  $$
  \|x\|_1 = \sum{i=1}^{n} |x_i|
  $$

  ​       在几何上，1-范数表示在一个坐标轴上移动的总距离。
  - $L_2$范数（欧几里得范数）：定义为向量各元素平方和的平方根。

  $$
  \|x\|_2 = \sqrt{\sum{i=1}^{n} |x_i|^2}
  $$

  ​       它对应的是向量的欧几里得距离，表示从原点到向量终点的直线距离。
  - $L_\infty$范数（最大范数）：定义为向量中元素的最大绝对值。

  $$
  \|x\|_\infty = \max{i} |x_i|
  $$

  

  ​       表示一个向量的“最远距离”，即向量中绝对值最大的那个分量。
  - $L_p$范数：是上述1-范数和2-范数的推广形式，定义为：

  $$
  \|x\|_p = \left( \sum{i=1}^{n} |x_i|^p \right)^{1/p}
  $$

  

  ​      其中  $p \geq 1$ 。

  - $L_0$范数，由对$L_p$范数的定义取极限 $p \to 0^+$ 得到，其结果是向量中不为 $0$ 的元素的个数：
    $$
    \lim_{p \to 0^+} \|x\|_p = \|x\|_0
    $$
    

    但虽然 $L_0$ 范数表示为向量中非零元素的个数，但它不严格满足向量范数的第二个性质，即齐次性。因为对于任意非零 $\alpha$ ，我们不能保证 $\|\alpha x\|_0 = |\alpha| \|x\|_0$ 成立。因此不被视为真正的范数。向量范数必须满足全部三个性质。



## 2.2 矩阵

### 2.2.1 矩阵的基本概念

- 矩阵与向量

  向量实际上是一种特殊的矩阵，列向量的列数为 1，行向量的行数为 1。同时，矩阵也可以看作由一组向量构成。设 $a_i = (a_{1i},a_{2i},...,a_{mi})^T$，那么 $A = (a_1,a_2,...,a_n)$

-  $n$ 阶方阵，对角矩阵，单位矩阵，$0$ 矩阵

  

  - 矩阵行数和列数均为的矩阵称为 $n$ 阶方阵

  

  - 如果 $n$ 方阵 $D$ 只有左上到右下的对角线上的元素不为 0，则称该方阵为对角矩阵（diagonal matrix），记为 $diag(a_1,a_2,\dots,a_n)$。例如 $diag(1,2,3)$ 代表的矩阵为：
    $$
    diag(1,2,3)=\begin{pmatrix}1&0&0\\0&2&0\\0&0&3\end{pmatrix}
    $$

    

  - 如果对角矩阵对角线上的元素全部为 1，则称该方阵为 $n$ 阶单位矩阵（identity matrix）或单位矩阵，用 $I_n$ 表示。阶数明确时，也可以省略下标的阶数，记为 $I$ 。例如，三阶单位阵为：

$$
  I_3 = \begin{pmatrix}
  1&0&0\\
  0&1&0\\
  0&0&1
  \end{pmatrix}
$$

  

  - 所有元素为零的矩阵称为零矩阵，记为 $0$



### 2.2.2 矩阵与线性变换

**矩阵**表示一种线性变换，

**线性变换**是什么，线性就是让网格保持平行且等距分布，所有直线还是直线，变换就是作用与空间（或空间向量）的一个函数，使它从之前的位置，变换到，之后的位置

为了解释这些，我们从最直观的最常见的直角坐标系讲起

**直角坐标系**，并不是像中学那样默认的，肯定的，它也是有定义的，有意义的，即由相互垂直的**基向量**组成，通俗来说，空间中的所有点或者说向量，都由基向量进行**线性组合**得到，这个空间称为基向量的线性组合的集合张成的空间，而直角坐标系所表示的空间是只是其中一种。

我们常常用 $(x, y, z, ...)$ 来表示一个向量，但我们都建立在默认这个向量存在于直角坐标系里面

如果我们跳出这个其他人设定的默认，那在直角坐标系里的向量应该表示为：
$$
\begin{pmatrix}
1&0&0&...\\
0&1&0&...\\
0&0&1&...\\
\vdots&\vdots&\vdots
\end{pmatrix}\begin{pmatrix}x\\y\\z\\ \vdots\end{pmatrix}
$$
所以，存在于直角坐标系里面的向量可以理解为一个变换前后完全一致的线性变换作用于一个向量上（即乘以一个单位矩阵），它本质上是运动的，是变化的，而非中学课本上所感受到的好像是静止不动的一条条所谓的“有向线段” （之所以我们定义向量是有向线段，很大程度上是因为一般数学上一般研究的都是线性代数，不是非线性代数，所以一切都建立在线性下，“ 即让网格保持平行且等距分布，所有直线还是直线 ”，固然才定义其为线段）

那如果向量不是在直角坐标系里面呢？

答案显而易见，对直角坐标系这个空间进行线性变换 = 对表示直角坐标系这个空间的向量进行线性变换 = 对表示直角坐标系这个空间的基向量进行线性变换 = 即对表示直角坐标系的单位矩阵进行线性变换 = 用另一个矩阵与单位矩阵相互作用，而这个作用方式我们叫   **矩阵乘法** *【上面说到的   “ 存在于直角坐标系里面的向量可以理解为一个变换前后完全一致的线性变换作用于一个向量上（即乘以一个单位矩阵）”其实也是矩阵乘法，因为向量其实也是一个矩阵（省略了很多 0 没写罢了）】*

以一个三维空间为例，演示一次矩阵乘法

一个表示三维空间的直角坐标系下的矩阵：
$$
\begin{pmatrix}
1&0&0\\
0&1&0\\
0&0&1
\end{pmatrix}
$$
一个表示三维空间的一个非直角坐标系下的矩阵：
$$
\begin{pmatrix}
a&b&c\\
d&e&f
\end{pmatrix}
$$
相乘（式子里放在前面的矩阵作用于放在后面的矩阵）：
$$
\begin{pmatrix}
a&b&c\\
d&e&f
\end{pmatrix}\begin{pmatrix}1&0&0\\0&1&0\\0&0&1\end{pmatrix}=\begin{pmatrix}
a&b&c\\
d&e&f
\end{pmatrix}
$$

这样，我们就成功将直角坐标系变换成了一个非直角坐标系

同时，你也可以发现，其实任何矩阵都可以理解为是对直角坐标系的一次变换，且**每个矩阵的其中一列就是该变换后空间的一个基向量**，因为其就是对直角坐标系空间的基向量进行了线性变换后所得到的新的空间的基向量	

假设原空间的基向量是$\ \vec i ,\ \vec j,\ \vec t$，新空间的基向量是$\ \vec i_1 ,\ \vec j_1,\ \vec t_1$ :
$$
\begin{pmatrix}
a\\d
\end{pmatrix} = \vec i_1 , \ \
\begin{pmatrix}
b\\e
\end{pmatrix} = \vec j_1 \\ ,\ \ \begin{pmatrix}
c\\f
\end{pmatrix}= \vec t_1
$$

$$
\uparrow \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \uparrow \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \uparrow \  \ \ \ \ \ \ \ \
$$

$$
\begin{pmatrix}
1\\0\\0
\end{pmatrix} = \vec i , \ 
\begin{pmatrix}
0\\1\\0
\end{pmatrix} = \vec j \\ ,\ \ \begin{pmatrix}
0\\0\\1
\end{pmatrix}= \vec t_1
$$



这里你可能会注意到，**为什么矩阵乘法是这样进行计算的呢？**

我们来拆解一下下面这个矩阵乘法进行解释：
$$
\begin{pmatrix}
a&b&c\\
d&e&f
\end{pmatrix}\begin{pmatrix}1&1&1\\1&1&1\\1&1&1\end{pmatrix}=\begin{pmatrix}
a+b+c&a+b+c&a+b+c\\
d+e+f&d+e+f&d+e+f
\end{pmatrix}
$$
这个矩阵乘法本质上就是对由
$$
\begin{pmatrix}
a\\d
\end{pmatrix} = \vec i_1 , \ \
\begin{pmatrix}
b\\e
\end{pmatrix} = \vec j_1 \\ ,\ \ \begin{pmatrix}
c\\f
\end{pmatrix}= \vec t_1
$$
这三个基向量所代表的一个空间对由
$$
\begin{pmatrix}
1\\1\\1
\end{pmatrix} = \vec i , \ 
\begin{pmatrix}
1\\1\\1
\end{pmatrix} = \vec j \\ ,\ \ \begin{pmatrix}
1\\1\\1
\end{pmatrix}= \vec t
$$
这三个基向量代表的空间的一次线性变换，所以
$$
\begin{pmatrix}
1\\1\\1
\end{pmatrix} = \vec i , \ 
\begin{pmatrix}
1\\1\\1
\end{pmatrix} = \vec j \\ ,\ \ \begin{pmatrix}
1\\1\\1
\end{pmatrix}= \vec t
$$
这三个基向量的需要分别与矩阵
$$
\begin{pmatrix}
a&b&c\\
d&e&f
\end{pmatrix}
$$
进行一次线性变换，即：
$$
\vec i_{new}=
\begin{pmatrix}
a&b&c\\
d&e&f
\end{pmatrix}\begin{pmatrix}1\\1\\1\end{pmatrix}
=\begin{pmatrix}
a\\d
\end{pmatrix} \times 1+
\begin{pmatrix}
b\\e
\end{pmatrix} \times 1 \\ + \begin{pmatrix}
c\\f
\end{pmatrix} \times 1 

=\begin{pmatrix}
a+b+c\\
d+e+f
\end{pmatrix}
$$

$$
\vec j_{new}=
\begin{pmatrix}
a&b&c\\
d&e&f
\end{pmatrix}\begin{pmatrix}1\\1\\1\end{pmatrix}
=\begin{pmatrix}
a\\d
\end{pmatrix} \times 1+
\begin{pmatrix}
b\\e
\end{pmatrix} \times 1 \\ + \begin{pmatrix}
c\\f
\end{pmatrix} \times 1 

=\begin{pmatrix}
a+b+c\\
d+e+f
\end{pmatrix}
$$

$$
\vec t_{new}=
\begin{pmatrix}
a&b&c\\
d&e&f
\end{pmatrix}\begin{pmatrix}1\\1\\1\end{pmatrix}
=\begin{pmatrix}
a\\d
\end{pmatrix} \times 1+
\begin{pmatrix}
b\\e
\end{pmatrix} \times 1 \\ + \begin{pmatrix}
c\\f
\end{pmatrix} \times 1 

=\begin{pmatrix}
a+b+c\\
d+e+f
\end{pmatrix}
$$

所以，
$$
(\vec i_{new} \ \ \vec j_{new} \ \ \vec t_{new}) = \begin{bmatrix}
\begin{pmatrix}
a+b+c\\d+e+f
\end{pmatrix}\begin{pmatrix}
a+b+c\\d+e+f
\end{pmatrix}\begin{pmatrix}
a+b+c\\d+e+f
\end{pmatrix}
\end{bmatrix}
\\ =\begin{pmatrix}
a+b+c&a+b+c&a+b+c\\
d+e+f&d+e+f&d+e+f
\end{pmatrix}
$$


知道了矩阵乘法原来是矩阵对原矩阵的每一列（即该空间的每一个基向量）进行线性变换，但你可能又会产生一个疑惑，为什么他们相乘是这样的呢：
$$
\begin{pmatrix}
a&b&c\\
d&e&f
\end{pmatrix}\begin{pmatrix}1\\1\\1\end{pmatrix}
=\begin{pmatrix}
a\\d
\end{pmatrix} \times 1+
\begin{pmatrix}
b\\e
\end{pmatrix} \times 1 \\ + \begin{pmatrix}
c\\f
\end{pmatrix} \times 1
$$
我们一步步来解释：

首先，我们观察一个方程组：
$$
ax+by=m\\
cx+dy=n
$$
你是否会觉得很眼熟？我们尝试改变一下写法：
$$
\begin{pmatrix}
a\\c
\end{pmatrix}x+\begin{pmatrix}
b\\d
\end{pmatrix}y=\begin{pmatrix}
m\\n
\end{pmatrix}
$$
这样写，就不难看出，**一个向量与一个矩阵相乘，实际上就是一个线性方程组又或者说就是两个函数**，这个结论很关键

因为其实函数是有维度的

可以将 $$y=f(x)$$ 看成是将 x 轴上的数据映射到了 y 轴上，这是一个从一维数轴的数据转换到另一个一维数轴上的对应关系。所以我认为这是一个一维函数。注意：这里将一维数轴看作和直角坐标轴有同等地位的一种坐标轴。

当然， $z=f(x,y)$ 可以看成是将一个二维数据降成了一维，这里 $(x,y)$ 表示一个二维数据， $z$ 表示一个一维数据。换句话说，这样的映射关系将一条二维信息转换为了一条一维信息，从几何角度说是，将一个平面“拍打“成了一条数轴。这个“拍打”（映射关系）我可以把它称之为“降维打击”，这就是为什么函数是有维度的

而有意思的是，从二维到二维，这个时候你会发现，必须要用到两个函数即一个方程组才能描述出来，比如我们上面所举的例子
$$
ax+by=m\\
cx+dy=n
$$
对于这个方程组，我们每给一个 $(x,y)$ ，都会收获一个 $(m,n)$ ，这是一个从二维数据到二维数据的过程，分析一下，会发现，其实 $m$ 是通过第一个方程的映射关系将二维数据 $(x,y)$ 下降成一维数据获得的，同样的， $n$ 是通过第二个方程的映射关系将同一个二维数据下降成一维获得的，然后将获得的两个一维数据组合成一个二维数据 $(m,n)$ .注意到，如果将 $(x,y)$ 看成是直角坐标的一个点，那么 $(m,n)$ 就是一个以 $(ax+by)$ 为横坐标， 以$(cx+d y)$ 为纵坐标的坐标系中的一个坐标。也就是讲：这个方程组的功能是将一个坐标系中的点转换到了另一个坐标系中的一个点。我们将这个方程组用矩阵的形式表达出来，也就是
$$
\begin{pmatrix}
a&b\\c&d
\end{pmatrix}
\begin{pmatrix}
x\\y
\end{pmatrix}

=\begin{pmatrix}
m\\n
\end{pmatrix}
$$
我们看着这个矩阵的形式，再重复一遍意思：将 $x , y$ 这个直角坐标系中的点 $(x,y)$ 映射到了另一个以 $ax+by$ 为横坐标， $cx+dy$ 为纵坐标的坐标系中去了，但由于表示该坐标系我们不做改变，所以这个被映射到新坐标系的向量用原坐标系表示为$(m,n)$。而这也就是矩阵对一个向量进行的一次线性变换。

综上所述，**我们为了简洁直观的表达空间的转换，我们引入了矩阵和向量，矩阵乘法等**，其中向量作为可视化的物品，矩阵作为线性转换的方式，**将也能表达这个意思但非常繁琐的函数，代数方程式替代**。这样，每当我们看到矩阵，向量，就能知道它们正在表达一些空间正在发生变化。从这个角度看，向量和矩阵是一项为了更方便研究数学的伟大发明，基于这项发明，发展出了一个学科分支：线性代数



### 2.2.3 矩阵运算

- 矩阵加法
- 矩阵数乘
- 矩阵乘法

这里你可以发现，矩阵的运算似乎和向量的运算很像，这是因为向量也是一种特殊的矩阵，**向量内积（或点积）其实是矩阵乘法的一种特殊形式**。所以矩阵的运算其实也是向量的运算，即**向量运算属于矩阵运算，是矩阵运算的一种特殊情况。**				

- **矩阵转置**（对偶变换）

与向量相似，矩阵同样存在转置操作，表示将矩阵的行和列交换。一个 $m\times n$ 矩阵转置后会得到 $n\times m$ 矩阵。例如，矩阵
$$
A= \begin{pmatrix}
1 & 2\\
3 & 4 \\
5 & 6 \\
\end{pmatrix}
$$


的转置为：
$$
A= \begin{pmatrix}
1 & 3 & 5\\
2 & 4 & 6
\end{pmatrix}
$$
​	



什么是对偶变换？它和转置有什么关系？

对偶变换：如果 $f$ 是 $W^*$ 中的一个线性函数，那么$ A^T f $是 $V^*    $中的一个线性函数，并且定义为：
$$
(A^T f)(v) = f(Av)
$$
什么是线性函数：一个线性函数 $f \in V^*     $可以表示为：
$$
f(\mathbf{x}) = a_1 x_1 + a_2 x_2
$$
其中， $a_1$ 和 $a_2$ 是实数系数， $\mathbf{x} = (x_1, x_2)$ 是  $V  $ 中的向量。这种线性函数对向量进行“测量”，可以理解为计算向量在某个方向上的投影，但这里的投影是通过线性组合来实现的。

例如：假设 $\mathbf{x} = (3, 4)$ 是一个二维向量，我们希望测量它在某个方向上的分量，比如沿着方向向量 $\mathbf{v} = (1, 0)$ （即 x 轴方向）。我们可以定义一个线性函数 $f \in V^*  $为：
$$
f(\mathbf{x}) = 1 \cdot 3 + 0 \cdot 4 = 3
$$
​	这个线性函数 $f$ 就是测量向量 $\mathbf{x}$ 在 $x$ 轴方向上的分量，结果是 $3$

这里你也可以看出，**线性函数，本质上就是进行了一次向量的点积**：
$$
\vec v \cdot\vec x =\left(1,0\right)\begin{pmatrix}3\\4\end{pmatrix}=3
$$
将一个向量从二维降到了一维（即一个点），其实也就是一次线性变换。而 $\vec v$ 也叫做这次线性变换的对偶向量。

那如果我们对向量 $\vec x$ 进行了一次线性变换：$ Ax$ 呢？

当对向量 $\vec x$ 进行了一次线性变换：$ Ax$​ ，为了使得：
$$
\mathbf{v} \cdot (A \mathbf{x}) = \mathbf{v}{\prime} \cdot \mathbf{x}
$$
其中 $\mathbf{v}{\prime}$​ 是变换后的新的对偶向量。我们引入了矩阵的转置，让 $\mathbf{v}{\prime}$ 满足：
$$
\mathbf{v}{\prime} = A^T \mathbf{v}
$$
这里， $A^T$ 就表示矩阵 $A$ 的转置矩阵，这意味着当我们对向量 $\mathbf{x}$ 进行某个线性变换时，即线性变换 $A$ 将向量 $\mathbf{u}$ 变换为新的向量 $A \mathbf{u}$ 。相应的“对偶向量”也需要以某种方式调整，以适应这种变换，即 $A^T \mathbf{v} $。这样才能在变换中保持“内积结构”，即让 
$$
\mathbf{v} \cdot (A \mathbf{x}) = (A^T \mathbf{ v}  ) \cdot \mathbf{x}
$$
成立

这便是矩阵转置的意义

另外，在矩阵乘法中，对于矩阵操作，有 $(AC)^T = C^T A^T$ 

可以看作是对复合线性变换的顺序反转。当我们进行一个复合线性变换 $AC$ 时，实际上是先进行 $C$ 的变换，然后进行 $A$ 的变换。而当我们取转置时，这些变换的顺序会反过来，因此在转置后得到的是 $C^T A^T$



- **矩阵的秩** = 基向量的数量 = 维度

- **矩阵的逆**

对于一个 $n \times n$ 的方阵 A ，它可逆的条件是存在一个矩阵 $A^{-1}$ 使得：
$$
A A^{-1} = I \quad \text{和} \quad A^{-1} A = I
$$
其中 $I$ 是 $n \times n$ 的单位矩阵。为了满足这个条件，矩阵 A 必须是**满秩矩阵**，也就是秩 $\text{rank}(A) = n$ 

*为什么满秩矩阵是可逆的?*

对于一个 $n \times n$ 的方阵 $A$ ：

当 $\text{rank}(A) = n$ 时，矩阵的行（或列）向量是线性独立的，这意味着矩阵可以通过线性变换将整个 $n$ 维空间映射到另一个 $n$ 维空间，且不会压缩到低维空间。这种情况下，线性变换是“可逆的”，因为没有任何维度被丢失，原来的向量可以通过某个逆变换（即 $A^{-1}$ ）恢复回来。

让我们通过一个具体的例子来说明，当矩阵的秩小于 n 时，矩阵的线性变换会将某些向量压缩到低维空间。

考虑一个 $2 \times 2$ 的矩阵：
$$
A = \begin{bmatrix} 1 & 1 \\ 2 & 2 \end{bmatrix}
$$
我们来分析这个矩阵的秩以及它对向量的线性变换。

1. 计算矩阵的秩

矩阵 A 的两行是 (1, 1) 和 (2, 2) 。这两行是线性相关的，因为第二行是第一行的 2 倍。这意味着矩阵的行秩为 1，而不是 2。类似地，它的列向量 (1, 2) 和 (1, 2) 也是线性相关的，因此矩阵的列秩也为 1。

因此，矩阵 A 的秩为 1，小于其维数 n = 2 。

2. 线性变换的几何意义

当我们用这个矩阵 A 作用于一个二维向量 $\mathbf{x} = (x_1, x_2)$ 时，得到的结果是：
$$
A \mathbf{x} = \begin{bmatrix} 1 & 1 \\ 2 & 2 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} x_1 + x_2 \\ 2x_1 + 2x_2 \end{bmatrix} = (x_1 + x_2) \begin{bmatrix} 1 \\ 2 \end{bmatrix}
$$
结果表明，任意向量 $\mathbf{x}$ 经过矩阵 A 的变换后，都被映射到与 (1, 2) 共线的向量。这意味着所有的输入向量 $\mathbf{x}$ 都被压缩到一个一维空间上，即沿着 (1, 2) 的方向。这就是所谓的将向量“压缩到低维空间”的现象。

- 行列式

  $det\begin{pmatrix}a&b\\c&d \end{pmatrix}=ad-bc$

如果你还记得的话，行列式是这么计算的

下面告诉你，为什么是这么算？它表达了是什么？

行列式的绝对值，表达的是线性变换（矩阵运算）后，空间缩放的比例（The determinant of a transformation）。

行列式的正负，表示空间有没有发生翻转。当行列式是负数时，空间发生翻转（也可以叫作空间发生了变向，起初 $\vec i$ 在 $\vec j$ 右边，变换后， $\vec i$ 在 $\vec j$ 左边）

而当矩阵的行列式为0时，它表示：这个线性变换将空间压缩到更小的维度上。

三维空间中，矩阵行列式的绝对值，表示的是，单位平行六面体经过线性变换（矩阵）后的体积

- 向量叉积

  



### 2.2.4 矩阵范数

与向量类似，在矩阵上同样可以定义范数函数 $\| \cdot \| : \mathbb{R}^{m \times n} \to \mathbb{R}$ ，其需要满足的条件也与向量范数相同：

正定性： $\| A \| \geq 0 ，且 \| A \| = 0 当且仅当 A 的所有元素都为 $0

绝对齐次性： $\| aA \| = |a| \| A \|$

三角不等式： $\| A + B \| \leq \| A \| + \| B \|$

在机器学习中，较为常用的矩阵范数是弗罗贝尼乌斯范数（Frobenius norm），简称 $F$ 范数，定义为矩阵每个元素的平方之和的平方根：
$$
\| A \|F = \left( \sum_{i=1}^{m} \sum_{j=1}^{n} (a_{ij})^2 \right)^{\frac{1}{2}}
$$
 $F$ 范数与向量的 $L_2$ 范数的定义较为类似，直观来说，可以用来衡量矩阵整体的“大小”，或者可以理解为将矩阵拉成向量后，向量对应的模长



## 2.3 梯度

pass



## 2.4 凸函数

pass



# 第三章  k近邻算法

最基本的分类和回归算法：k 近邻（k-nearest neighbor, KNN）算法。KNN 是最简单也是最重要的机器学习算法之一，它的思想可以用一句话来概括：“相似的数据往往拥有相同的类别”，这也对应于中国的一句谚语：“物以类聚，人以群分”。具体来说，我们在生活中常常可以观察到，同一种类的数据之间特征更为相似，而不同种类的数据之间特征差别更大。



## 3.1 KNN算法的原理

我们用 KNN 算法的一张经典示意图来更清晰地说明其思想。如图所示，假设共有两个类别的数据点：蓝色圆形和橙色正方形，而中心位置的绿色样本当前尚未被分类。根据统计近邻的思路：

- 当时 $K = 3$，绿色样本的 $3$ 个近邻中有两个橙色正方形样本，一个蓝色圆形样本，因此应该将绿色样本点归类为橙色正方形。
- 当时 $K=5$，绿色样本的 $5$ 个近邻中有两个橙色正方形样本，三个蓝色圆形样本，因此应该将绿色样本点归类为蓝色圆形。

![截屏2024-10-23 21.43.38](/Users/aris/Library/Application Support/typora-user-images/截屏2024-10-23 21.43.38.png)

下面，我们用数学语言来描述 KNN 算法：

**第一步**，我们有一个已经训练好的数据集
$$
D = \{ (x_i, y_i) \mid i = 1, 2, \dots, N \}
$$
其中 $x_i$ 表示某一个样本数据点，$y_i$ 表示该样本数据点的类别标签（在回归问题时，$y_i$ 是一个实数，表示某一个类别）

**第二步**，我们设一个新的待分类或预测的样本 $x$ ，计算它与训练集 $D$ 里每一个样本的距离 $d(x_i,x)$ ，常用的距离度量是欧几里得距离，定义为：
$$
d(x_i, x) = \sqrt{\sum_{j=1}^d (x_{i,j} - x_j)^2}
$$
计算完成后，得到距离集合：
$$
\{ d(x_1, x), d(x_2, x), \dots, d(x_N, x) \} 
$$
**第三步**，从距离集合中选择距离最近的 $K$ 个样本，记作集合 $ N_K(x) $ ：
$$
N_K(x) = \{ (x_{i_1}, y_{i_1}), (x_{i_2}, y_{i_2}), \dots, (x_{i_K}, y_{i_K}) \}
$$
再者，对 $ N_K(x) $ 进行分类：
$$
\hat{y} = \underset{c \in C}{\arg\max} \sum_{(x_i, y_i) \in N_K(x)} \mathbb{I}(y_i = c)
$$
注意，这个 $c$ 可以是不同的值，即不同的类型

**第四步**，回归问题，对 $K$ 个邻居的标签取平均值作为预测值：
$$
\hat{y} = \frac{1}{K} \sum_{(x_i, y_i) \in N_K(x)} y_i
$$
当然，也可以根据数据集的特性设置权重与距离的关系，例如让权重与距离成反比；还可以将权重作为模型的参数，通过学习得到。此情况的预测值：
$$
\hat{y}(x) = \sum_{(x_i, y_i) \in N_K(x)} w_iy_i \ ,\ \  \sum_{i=1}^Kw_i=1
$$


从这个例子中可以看出，KNN 的基本思路是让当前样本的分类服从邻居中的多数分类。但是，当 $K$ 的大小变化时，由于邻居的数量变化，其多数类别也可能会变化，从而改变对当前样本的分类判断。因此，决定 $K$ 的大小是 KNN 中最重要的部分之一。直观上来说，当 $K$ 的取值太小时，分类结果很容易受到待分类样本周围的个别噪声数据影响；当 $K$ 的取值太大时，又可能将远处一些不相关的样本包含进来。因此，我们应该根据数据集动态地调整 $K$ 的大小，以得到最理想的结果。



## 3.2 用 KNN 算法完成分类任务





